\documentclass[conference]{IEEEtran}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}

\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Exploring Implicit and Explicit Parallelism with
OpenMP\\
{\footnotesize a.y. 2024/2025}
}

\author{\IEEEauthorblockN{ Luca Donatellis}
\IEEEauthorblockA{237802 \\
luca.donatellis@unitn.it}
}

\maketitle

\begin{abstract}
The goal of this project was to check if a square matrix of side n was symmetric, if not compute the transposition. The objective was to improve the performance of the algorithm and measure the execution time using various programming techniques, starting from the sequential execution to the implicit parallelism finally to the explicit parallelism with OpenMP. Through this process, we observed that each successive iteration of optimization significantly reduced the algorithm's execution time, demonstrating the effectiveness of parallelization in improving computational efficiency.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert??
\end{IEEEkeywords}

\section{Introduction of the problem and importance}
The matrix transposition algorithm is one of the most widely used algorithms in the world. It is essential in many different applications, ranging from machine learning, business, biology, physics,  weather forecasting, to space missions, among others. This is because matrix transposition is a fundamental operation in various fields where large matrices are frequently manipulated.
The goal of this project is to develop a matrix transposition algorithm with the least possible execution time, utilizing different approaches evaluating the performance of each of them to determine the most efficient method.

\section{State-of-the-art}
The study by Andrey Vladimirov focuses on optimizing the matrix transposition algorithm using C++ and the OpenMP library. The paper aims to achieve the best execution time for transposing a square matrix of size $n\times n$ on two specific architectures: Intel Xeon Processors and Intel Xeon Phi Coprocessors. The author provides a detailed analysis of various optimization techniques, including the use of tile-based blocking to improve memory access patterns and specific compiler flags to maximize performance.
While Vladimirov’s work demonstrates significant improvements in execution time, it is primarily designed for a specific type of hardware. Additionally, his work focuses on one solution without exploring other possible approaches.

\section{Contribution and Methodology}

This project adopts an approach similar to the one presented by Vladimirov but focuses on analyzing the impact of different parallelization techniques: serial, implicit, and explicit on matrix transposition. Unlike Vladimirov’s work, which targets achieving the highest possible performance on specific processors, this report aims to explore and compare the efficiency of various parallelization strategies on general-purpose hardware and the university computing cluster.

Each methodology required implementing a function "checkSym" to check whether a given matrix \( M \) is symmetric and another function "matTranspose" to store the transpose of \( M \) in a separate matrix \( T \). To ensure consistency and enable later comparisons, none of these functions terminates early; instead, they evaluate all elements of the matrix even if asymmetry or other criteria are detected partway through.
\subsection{Matrix Representation}  
The matrices is represented as a dynamically allocated 2D array (\texttt{float**}). Each row is stored as a separate pointer, providing flexibility in memory allocation and enabling efficient access to elements using \texttt{M[i][j]}.

\subsection{Serial Methodology}
The serial implementation employs a straightforward algorithm, designed primarily to measure execution time and serve as a baseline for parallel implementations. 

\paragraph{Symmetry Check}  
The \texttt{checkSym} function determines whether the matrix \( M \) is symmetric by iterating over its upper triangle and comparing each element with its corresponding entry in the lower triangle.
\begin{algorithm}
\caption{\texttt{checkSym}}
\begin{algorithmic}[1]
\State $sym \gets \texttt{true}$
\For{$i = 0$ \textbf{to} $n-2$}
    \For{$j = i+1$ \textbf{to} $n-1$}
        \If{$M[i][j] \neq M[j][i]$}
            \State $sym \gets \texttt{false}$
        \EndIf
    \EndFor
\EndFor
\State \Return $sym$
\end{algorithmic}
\end{algorithm}

\paragraph{Matrix Transposition}  
The \texttt{matTranspose} function computes the transpose of \( M \) by swapping rows and columns. The transposed elements are stored in a separate matrix \( T \).
\begin{algorithm}
\caption{\texttt{matTranspose}}
\begin{algorithmic}[1]
\For{$i = 0$ \textbf{to} $n-1$}
    \For{$j = 0$ \textbf{to} $n-1$}
        \State $T[j][i] \gets M[i][j]$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\paragraph{Challenges and Solutions}
\begin{itemize}
    \item \textbf{Matrix size input:}  
    \textit{Challenge:} Getting the size of the matrix as an argument.
    \textit{Solution:} Using the \texttt{**argv} array to get the argument then parse it into an integer.
    \item \textbf{Memory Allocation:}  
    \textit{Challenge:} Creating an \( n \times n \) matrix based on the input value \( n \).  
    \textit{Solution:} Allocating the matrix dynamically using a pointer-to-pointer structure to handle varying sizes at runtime. 
\end{itemize}

\subsection{Implicit Methodology}
The implicit parallelization implementation takes advantages of:\begin{itemize}\item \textbf{SIMD} (Single Instruction Multiple Data) capabilities provided by OpenMP to improve the performance of matrix operations. By focusing on vectorization, this approach enables simultaneous computation on multiple elements within a loop, optimizing performance without introducing explicit threading. 
\item \textbf{Data locality}
dividing the matrix in blocks of size \( \texttt{BLOCK\_SIZE} \) to enhance data locality and minimize cache misses. 
\end{itemize}
\paragraph{Symmetry Check} 
The \texttt{checkSymImp} function determines whether the matrix \( M \) is symmetric by iterating over its upper triangle in blocks for data locality and comparing each element with its corresponding entry in the lower triangle.
The use of \texttt{\#pragma omp simd} ensures that comparisons within each block are vectorized, improving execution speed.
\begin{algorithm}
\caption{\texttt{checkSymImp}}
\begin{algorithmic}[1]
\State $blockSize \gets \min(n, 64)$
\State $sym \gets \texttt{true}$
\For{$i = 0$ \textbf{to} $n-1$ \textbf{step} $blockSize$}
    \For{$j = i$ \textbf{to} $n-1$ \textbf{step} $blockSize$}
        \For{$y = i$ \textbf{to} $\i+blockSize-1$}
            \State \textbf{\#pragma omp simd}
            \For{$x = j$ \textbf{to} $j+blockSize-1$}
                \If{$M[x][y] \neq M[y][x]$}
                    \State $sym \gets \texttt{false}$
                \EndIf
            \EndFor
        \EndFor
    \EndFor
\EndFor
\State \Return $sym$
\end{algorithmic}
\end{algorithm}
\paragraph{Matrix Transposition} 
The \texttt{matTransposeImp} function follows a similar blocking strategy. Each block of the matrix is transposed independently, storing the result in the matrix \( T \). Vectorization is achieved using \texttt{\#pragma omp simd} within the innermost loop, which swaps rows and columns efficiently.
\begin{algorithm}
\caption{\texttt{matTransposeImp}}
\begin{algorithmic}[1]
\State $blockSize \gets \min(n, 64)$
\For{$i = 0$ \textbf{to} $n-1$ \textbf{step} $blockSize$}
    \For{$j = 0$ \textbf{to} $n-1$ \textbf{step} $blockSize$}
        \For{$x = i$ \textbf{to} $i+blockSize-1$}
            \State \textbf{\#pragma omp simd}
            \For{$y = j$ \textbf{to} $j+blockSize-1$}
                \State $T[y][x] \gets M[x][y]$
            \EndFor
        \EndFor
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\paragraph{Challenges and Solutions}  

\begin{itemize}
    \item \textbf{Ensuring Vectorization:}  
    \textit{Challenge:} Ensuring that the compiler effectively vectorizes the inner loops.  
    \textit{Solution:} Using \texttt{\#pragma omp simd} explicitly instructs the compiler to vectorize the operations, even in the presence of potential memory aliasing.

    \item \textbf{Cache Efficiency}  
    \textit{Challenge:} Large matrices can cause frequent cache misses, slowing down the execution.  
    \textit{Solution:} Dividing the matrix into smaller blocks (\( \texttt{BLOCK\_SIZE} \)) ensures that frequently accessed data stays in the cache, improving performance.

    \item \textbf{Compiler error on the cluster}  
    \textit{Challenge:} The code using one \texttt{\#pragma omp simd} instruction for every for loop doesn't compile on the cluster.
    \textit{Solution:} Using only one \texttt{\#pragma omp simd} instruction just for the higher computational cost loop.
    \item \textbf{Bound out of memory access:}  
\textit{Challenge:}If the size of the matrix is not both a multiple of  \texttt{BLOCK\_SIZE} and greater than that access out of bound occurs. 
\textit{Solution:} By constrains the size of the matrix is a power of 2 so the problem doesn't persists, for a size smaller than  \texttt{BLOCK\_SIZE} use instead the size of the matrix.
\end{itemize}
\subsection{onlyOMP Methodology} 
The intermediate approach, called \textit{onlyOMP}, leverages: \begin{itemize} \item \textbf{Explicit Parallelism using OpenMP:} This method introduces explicit parallel threading with OpenMP's \texttt{\#pragma omp parallel for}, ensuring that computations are distributed across available threads. \item \textbf{Collapsing Nested Loops:} By collapsing nested loops into a single parallelized loop, \texttt{\#pragma omp collapse(2)}, we achieve a balanced workload across threads. \end{itemize}

\paragraph{Symmetry Check} The \texttt{checkSymOMP} function determines whether the matrix 
M is symmetric. Parallelism is achieved using \texttt{\#pragma omp parallel for collapse(2)}, which spreads iterations of the two nested loops across multiple threads. To ensure thread-safe updates to the result variable, the reduction clause is employed.

\begin{algorithm} \caption{\texttt{checkSymOMP}} \begin{algorithmic}[1] \State $sym \gets \texttt{true}$ \State \textbf{\#pragma omp parallel for collapse(2) reduction(\&\&:sym)} \For{$i = 0$ \textbf{to} $n-2$} \For{$j = i+1$ \textbf{to} $n-1$} \If{$M[i][j] \neq M[j][i]$} \State $sym \gets \texttt{false}$ \EndIf \EndFor \EndFor \State \Return $sym$ \end{algorithmic} \end{algorithm}

\paragraph{Matrix Transposition} The \texttt{matTransposeOMP} function transposes the matrix M into T using explicit parallelism. The loops are collapsed to enable parallel computation of individual elements, ensuring high performance and thread-level scalability.

\begin{algorithm} \caption{\texttt{matTransposeOMP}} \begin{algorithmic}[1] \State \textbf{\#pragma omp parallel for collapse(2)} \For{$i = 0$ \textbf{to} $n-1$} \For{$j = 0$ \textbf{to} $n-1$} \State $T[j][i] \gets M[i][j]$ \EndFor \EndFor \end{algorithmic} \end{algorithm}

\paragraph{Challenges and Solutions}

\begin{itemize} \item \textbf{Thread Safety in Symmetry Check:}
\textit{Challenge:} The symmetry check involves a shared variable 
sym, which can cause race conditions in a parallel loop.
\textit{Solution:} The use of \texttt{reduction(\&\&:sym)} ensures thread-safe updates by performing a reduction operation after all threads complete their iterations.
\item \textbf{Efficient Loop Parallelization:}  
\textit{Challenge:} Nested loops are typically harder to parallelize due to workload imbalance or scheduling overhead.  
\textit{Solution:} The \texttt{collapse(2)} directive in OpenMP allows the two loops to be treated as a single loop, ensuring a balanced distribution of iterations across threads.

\item \textbf{Scalability with Increasing Matrix Sizes:}  
\textit{Challenge:} Larger matrices can result in workload imbalances between threads.  
\textit{Solution:} OpenMP's dynamic scheduling (optional) or loop collapse ensures that the workload is distributed dynamically across threads.
\end{itemize}

\subsection{Explicit Methodology} The explicit parallelization implementation introduces direct thread-level management as in onlyOMP, leveraging OpenMP's capabilities to achieve fine-grained control over matrix operations.

\begin{itemize} \item \textbf{Thread-Level Control:} The implementation explicitly divides the work among threads using OpenMP's \texttt{\#pragma omp parallel for} directive. Each thread is assigned a specific subset of the matrix to process. \item \textbf{Blocked Iterations:} Iteration over the matrix is split into smaller blocks of size 
BLOCK\_SIZE, optimizing cache usage while ensuring even distribution of work among threads. \end{itemize}

\paragraph{Symmetry Check}
The \texttt{checkSymExplicit} function determines whether the matrix 
M is symmetric. Explicit thread-level parallelism is achieved by manually dividing the matrix blocks among threads. Each thread performs comparisons on its assigned blocks, updating a shared variable sym using \texttt{reduction(\&\& : sym)} directive to prevent race conditions.

\begin{algorithm} \caption{\texttt{checkSymExplicit}} \begin{algorithmic}[1] \State $sym \gets \texttt{true}$ \State $blockSize \gets \min(n, \texttt{BLOCK\_SIZE})$ \State \textbf{\#pragma omp parallel shared(sym)} \For{$i = 0$ \textbf{to} $n$ \textbf{step} $blockSize$} \For{$j = i$ \textbf{to} $n$ \textbf{step} $blockSize$} \For{$y = i$ \textbf{to} $i + blockSize - 1$} \For{$x = j$ \textbf{to} $j + blockSize - 1$} \State \textbf{\#pragma omp critical} \If{$M[x][y] \neq M[y][x]$} \State $sym \gets \texttt{false}$ \EndIf \EndFor \EndFor \EndFor \EndFor \State \Return $sym$ \end{algorithmic} \end{algorithm}

\paragraph{Matrix Transposition}
The \texttt{matTransposeExplicit} function assigns matrix blocks to threads for transposition. Each thread processes its designated blocks independently, writing to the output matrix 
T. This approach avoids data races by ensuring that threads operate on disjoint memory regions.

\begin{algorithm} \caption{\texttt{matTransposeExplicit}} \begin{algorithmic}[1] \State $blockSize \gets \min(n, \texttt{BLOCK\_SIZE})$ \State \textbf{\#pragma omp parallel} \For{$i = 0$ \textbf{to} $n$ \textbf{step} $blockSize$} \For{$j = 0$ \textbf{to} $n$ \textbf{step} $blockSize$} \For{$x = i$ \textbf{to} $i + blockSize - 1$} \For{$y = j$ \textbf{to} $j + blockSize - 1$} \State $T[y][x] \gets M[x][y]$ \EndFor \EndFor \EndFor \EndFor \end{algorithmic} \end{algorithm}

\paragraph{Challenges and Solutions}

\begin{itemize} 

\end{itemize}

\section{Experiments and System Description}
\subsection{Detailed description of the computing system and platform}
The codes were tested on two machines:
\begin{itemize} \item \textbf{Laptop:}
\begin{itemize} \item Operating system: Windows 11 Home
 \item Processor: AMD Ryzen™ 5 5600H (up to 4.2 GHz max boost clock, 6 cores, 12 threads)
 \item Cache: 384 KB L1 cache, 3 MB L2 cache, 16 MB L3 cache
 \item Chipset: AMD Integrated SoC
\item Memory: 2x8 GB DDR4-3200 MHz RAM, Transfer rates up to 3200 MT/s.
\item DataWidth: 64 bit
\end{itemize}
 \item \textbf{Laptop:}
\begin{itemize} \item Operating system: 
 \item Processor: 
 \item Cache: 
 \item Chipset: 
\item Memory: 
\item DataWidth: 
\end{itemize}
\end{itemize}
\subsection{Relevant specifications or configurations}
There were used only 3 libraries:
\begin{itemize} \item \textbf{iostream:} for output
 \item \textbf{omp.h:} for parallelization
 \item \textbf{string:} to parse argument into an integer
\end{itemize}

\subsection{Description of the experimental setup, procedures, and methodologies used in the project}
The tests were made using a bat script on Windows and using different pbs files on Linux, both the script tests the each methodology with a matrix size from $2^4$ to $2^12$ 5 time each, then the output is simplified by a program in ruby then manually checked to avoid outliers.
\subsection{Discussion on how experiments are designed to test the hypotheses or achieve the objectives}
\section{Results and Discussion}
\subsection{Presentation of results}
\subsection{Analysis and interpretation in context}
\subsection{Comparison with the state-of-the-art}
\section{Conclusions}
%Summary of key findings and contributions

\begin{thebibliography}{00}
\bibitem{b1}A. Vladimirov, "Multithreaded Transposition of Square Matrices with Common Code for Intel Xeon Processors and Intel Xeon Phi Coprocessors", August 2013
\end{thebibliography}